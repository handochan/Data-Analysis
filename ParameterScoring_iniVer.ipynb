{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCYcI+2hbErpZ5AWp39hOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/handochan/Data-Analysis/blob/main/ParameterScoring_iniVer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.api import OLS, add_constant\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import linregress\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Helper function for mean handling missing values\n",
        "def meanXNA(series):\n",
        "    return series.dropna().mean()\n",
        "\n",
        "# Error grade function\n",
        "def error_grade(ind_name):\n",
        "    tmp = {\n",
        "        \"X\": [ind_name],\n",
        "        \"pval\": [1],\n",
        "        \"slope\": [0],\n",
        "        \"pval.L\": [1],\n",
        "        \"slope.L\": [0],\n",
        "        \"pval.WL\": [1],\n",
        "        \"slope.WL\": [0],\n",
        "        \"Class\": [\"\"],\n",
        "        \"Grade\": [0]\n",
        "    }\n",
        "    return pd.DataFrame(tmp)\n",
        "\n",
        "\n",
        "def correlation(x, y):\n",
        "    \"\"\"\n",
        "    Perform linear regression and return key metrics.\n",
        "\n",
        "    Parameters:\n",
        "        x (array-like): Independent variable.\n",
        "        y (array-like): Dependent variable.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing p-value, r, r^2, slope, intercept, and degrees of freedom.\n",
        "    \"\"\"\n",
        "    # Convert to numpy arrays for better performance\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Filter out NaN values\n",
        "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
        "    x = x[valid]\n",
        "    y = y[valid]\n",
        "\n",
        "    # Check if there is enough data for regression\n",
        "    n = len(x)\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Not enough data points for regression.\")\n",
        "\n",
        "    # Perform linear regression\n",
        "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "\n",
        "    # Compute additional metrics\n",
        "    r_squared = r_value**2\n",
        "    degrees_of_freedom = n - 2  # For simple linear regression\n",
        "\n",
        "    # Package results into a dictionary\n",
        "    results = {\n",
        "        \"slope\": slope,\n",
        "        \"intercept\": intercept,\n",
        "        \"r\": r_value,\n",
        "        \"r_squared\": r_squared,\n",
        "        \"p_value\": p_value,\n",
        "        \"degrees_of_freedom\": degrees_of_freedom\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "# CalcGrade function\n",
        "def calc_grade(df_ind_resp, ind_name, lot_col, wf_col, response_col, factor_col=None):\n",
        "    \"\"\"\n",
        "    Calculate grade based on statistical correlations between variables.\n",
        "\n",
        "    Parameters:\n",
        "        df_ind_resp (pd.DataFrame): Input DataFrame.\n",
        "        ind_name (str): Name of the independent variable.\n",
        "        lot_col (str): Column name for lot identifier.\n",
        "        wf_col (str): Column name for weight factor (not used directly in this implementation).\n",
        "        response_col (str): Name of the response variable.\n",
        "        factor_col (str, optional): Name of the factor column for additional grouping.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing grades and statistical information.\n",
        "    \"\"\"\n",
        "    df_tmp_f = df_ind_resp.copy()\n",
        "\n",
        "    # Adjust for factors if provided\n",
        "    if factor_col is not None:\n",
        "        df_fac = df_tmp_f.groupby(factor_col).agg({\n",
        "            response_col: meanXNA,\n",
        "            ind_name: meanXNA\n",
        "        }).reset_index().rename(columns={response_col: \"facResp\", ind_name: \"facInd\"})\n",
        "\n",
        "        df_tmp_f = df_tmp_f.merge(df_fac, on=factor_col)\n",
        "        df_tmp_f[ind_name] -= df_tmp_f[\"facInd\"]\n",
        "        df_tmp_f[response_col] -= df_tmp_f[\"facResp\"]\n",
        "        df_tmp_f[lot_col] = df_tmp_f[lot_col] + \" \" + df_tmp_f[factor_col].astype(str)\n",
        "\n",
        "    # Aggregate by lot\n",
        "    df_lot = df_tmp_f.groupby(lot_col).agg({\n",
        "        response_col: meanXNA,\n",
        "        ind_name: meanXNA\n",
        "    }).reset_index().rename(columns={response_col: \"lotResp\", ind_name: \"lotInd\"})\n",
        "\n",
        "    if df_tmp_f[ind_name].nunique() == 1:\n",
        "        return error_grade(ind_name)\n",
        "\n",
        "    df_tmp_f = df_tmp_f.merge(df_lot, on=lot_col)\n",
        "    df_tmp_f[\"wLotInd\"] = df_tmp_f[ind_name] - df_tmp_f[\"lotInd\"]\n",
        "    df_tmp_f[\"wLotResp\"] = df_tmp_f[response_col] - df_tmp_f[\"lotResp\"]\n",
        "\n",
        "    # Perform linear regressions\n",
        "    try:\n",
        "        lot_results = correlation(df_lot[\"lotInd\"], df_lot[\"lotResp\"])\n",
        "        wlot_results = correlation(df_tmp_f[\"wLotInd\"], df_tmp_f[\"wLotResp\"])\n",
        "        overall_results = correlation(df_tmp_f[ind_name], df_tmp_f[response_col])\n",
        "    except ValueError:\n",
        "        return error_grade(ind_name)\n",
        "\n",
        "    # Extract key metrics\n",
        "    slope_lot, pval_lot, rsq_lot = lot_results[\"slope\"], lot_results[\"p_value\"], lot_results[\"r_squared\"]\n",
        "    slope_wlot, pval_wlot, rsq_wlot = wlot_results[\"slope\"], wlot_results[\"p_value\"], wlot_results[\"r_squared\"]\n",
        "    slope_w, pval_w = overall_results[\"slope\"], overall_results[\"p_value\"]\n",
        "\n",
        "    sigma_lot = np.sqrt(df_lot[\"lotInd\"].var())\n",
        "    sigma_wlot = np.sqrt(df_tmp_f[\"wLotInd\"].var())\n",
        "    range_ratio = 2 * sigma_wlot / (sigma_lot + 2 * sigma_wlot)\n",
        "\n",
        "    if np.isnan(slope_wlot * slope_lot * pval_lot * pval_wlot):\n",
        "        return error_grade(ind_name)\n",
        "\n",
        "    if slope_wlot * slope_lot > 0 and pval_lot < 0.2 and pval_wlot < 0.2:\n",
        "        apval_lot = min(1, pval_lot / 0.2)\n",
        "        apval_wlot = min(1, pval_wlot / 0.2)\n",
        "        arange_ratio = min(2 / 3, max(1 / 3, range_ratio))\n",
        "        sig_grade = 1 - (apval_lot * (1 - arange_ratio) + apval_wlot * arange_ratio)\n",
        "        orig_grade = 1 - max(pval_lot, pval_wlot)\n",
        "    else:\n",
        "        sig_grade = 0\n",
        "        apval_lot = min(1, pval_lot / 0.1)\n",
        "        apval_wlot = min(1, pval_wlot / 0.1)\n",
        "        if range_ratio < 0.2:\n",
        "            sig_grade = min(1, (1 - apval_lot) * 2 * rsq_lot)\n",
        "        elif range_ratio > 0.8:\n",
        "            sig_grade = min(1, (1 - apval_wlot) * 5 * rsq_wlot)\n",
        "        if pval_lot < 0.02 or pval_wlot < 0.02:\n",
        "            orig_grade = 1 - max(pval_lot, pval_wlot)\n",
        "\n",
        "    # Prepare result\n",
        "    tmp = {\n",
        "        \"X\": ind_name,\n",
        "        \"pval\": pval_w,\n",
        "        \"slope\": slope_w,\n",
        "        \"pval.L\": pval_lot,\n",
        "        \"slope.L\": slope_lot,\n",
        "        \"pval.WL\": pval_wlot,\n",
        "        \"slope.WL\": slope_wlot,\n",
        "        \"Class\": \"\",\n",
        "        \"Grade\": sig_grade\n",
        "    }\n",
        "\n",
        "    return tmp\n",
        "\n",
        "\n",
        "\n",
        "def CheckCorrGrade(df_ind, ind_name, lot_col, wf_col, response_col, factors=[\"UniqueModule\", \"TimePeriod\"], cutoff_p_val=0.05):\n",
        "    # Filter dataframe\n",
        "    df_ind_resp = df_ind[df_ind[ind_name].notna()][[lot_col, wf_col, response_col, ind_name] + factors]\n",
        "\n",
        "    if df_ind_resp.empty:\n",
        "        base_stats = error_grade(ind_name)\n",
        "        base_stats[\"Class\"] = \"\"\n",
        "        base_stats[\"Note\"] = \"\"\n",
        "        base_stats[\"base_grade\"] = 0\n",
        "        base_stats[\"adj_grade\"] = 0\n",
        "        base_stats[\"best_grade\"] = 0\n",
        "        return base_stats\n",
        "\n",
        "    factor_col = None\n",
        "    base_stats = calc_grade(df_ind_resp, ind_name, lot_col, wf_col, response_col, factor_col)\n",
        "    base_grade = base_stats[\"Grade\"]\n",
        "    adj_grade = base_grade\n",
        "    best_grade = base_grade\n",
        "\n",
        "    # print(base_grade)\n",
        "\n",
        "    # Determine classification\n",
        "    if base_grade > 0.5:\n",
        "        grade_class = \"Strong\"\n",
        "    elif base_grade > 0:\n",
        "        grade_class = \"Marginal\"\n",
        "    else:\n",
        "        grade_class = \"\"\n",
        "\n",
        "    note = \"\"\n",
        "\n",
        "    # Iterate through factors\n",
        "    for factor_col in factors:\n",
        "        fact_stats = calc_grade(df_ind_resp, ind_name, lot_col, wf_col, response_col, factor_col)\n",
        "        fact_grade = fact_stats[\"Grade\"]\n",
        "        adj_grade = min(adj_grade, fact_grade)\n",
        "        best_grade = max(best_grade, fact_grade)\n",
        "\n",
        "        if fact_grade < 0.5 * base_grade:\n",
        "            if grade_class:\n",
        "                grade_class += \"/\"\n",
        "            grade_class += \"Confounded\"\n",
        "            note += f\" Confounded by {factor_col}\"\n",
        "\n",
        "        if fact_grade > 2 * (0.1 + base_grade):\n",
        "            if grade_class:\n",
        "                grade_class += \"/\"\n",
        "            grade_class += \"Disguised\"\n",
        "            note += f\" Disguised by {factor_col}\"\n",
        "\n",
        "    # Update base stats\n",
        "    base_stats[\"Class\"] = grade_class\n",
        "    base_stats[\"Note\"] = note\n",
        "    base_stats[\"base_grade\"] = base_grade\n",
        "    base_stats[\"adj_grade\"] = adj_grade\n",
        "    base_stats[\"best_grade\"] = best_grade\n",
        "\n",
        "    # Compute final grade\n",
        "    final_grade = best_grade\n",
        "    if final_grade > 0:\n",
        "        final_grade = (0.5 + final_grade * 0.5) ** 2\n",
        "\n",
        "    # Normalize by \"UniqueModule\"\n",
        "    if final_grade > 0:\n",
        "        df_tmp = df_ind_resp.copy()\n",
        "        df_tmp = NormalizeByFactor(df_tmp, response_col, ind_name, \"UniqueModule\")\n",
        "        mult_factor = SlopeMultiplier(df_tmp, response_col, ind_name, \"UniqueModule\", None)\n",
        "        final_grade *= mult_factor\n",
        "\n",
        "        # Normalize by \"TimePeriod\"\n",
        "        df_tmp = NormalizeByFactor(df_tmp, response_col, ind_name, \"TimePeriod\")\n",
        "        mult_factor = SlopeMultiplier(df_tmp, response_col, ind_name, \"TimePeriod\", None)\n",
        "        final_grade *= mult_factor\n",
        "\n",
        "    base_stats[\"Grade\"] = final_grade\n",
        "\n",
        "    return base_stats\n",
        "\n",
        "\n",
        "# NormalizeByFactor function\n",
        "def NormalizeByFactor(df_tmp, resp_col, ind_name, factor):\n",
        "    # Aggregate mean values\n",
        "    df_fac = df_tmp.groupby(factor).agg(\n",
        "        facResp=(resp_col, meanXNA),\n",
        "        facInd=(ind_name, meanXNA)\n",
        "    ).reset_index()\n",
        "\n",
        "    # Adjust data types\n",
        "    if pd.api.types.is_numeric_dtype(df_tmp[factor]):\n",
        "        df_fac[factor] = pd.to_numeric(df_fac[factor])\n",
        "    else:\n",
        "        df_fac[factor] = df_fac[factor].astype(str)\n",
        "\n",
        "    # Merge and normalize\n",
        "    df_tmp = pd.merge(df_tmp, df_fac, how=\"left\", on=factor)\n",
        "    df_tmp[ind_name] = df_tmp[ind_name] - df_tmp[\"facInd\"]\n",
        "    df_tmp[resp_col] = df_tmp[resp_col] - df_tmp[\"facResp\"]\n",
        "\n",
        "    return df_tmp\n",
        "\n",
        "# SlopeMultiplier function\n",
        "def SlopeMultiplier(df_tmp, resp_col, ind_name, factor, pres=None):\n",
        "    df_tmp_f = df_tmp.copy()\n",
        "    df_tmp_f[\"Lot\"] = df_tmp_f[\"Lot\"].astype(str) + \"_\" + df_tmp_f[factor].astype(str)\n",
        "\n",
        "    # Aggregate mean values by \"Lot\"\n",
        "    df_lot = df_tmp_f.groupby(\"Lot\").agg(\n",
        "        lotResp=(resp_col, meanXNA),\n",
        "        lotInd=(ind_name, meanXNA)\n",
        "    ).reset_index()\n",
        "\n",
        "    # Merge aggregated data\n",
        "    df_tmp_f = pd.merge(df_tmp_f, df_lot, how=\"left\", on=\"Lot\")\n",
        "    df_tmp_f[\"wLotInd\"] = df_tmp_f[ind_name] - df_tmp_f[\"lotInd\"]\n",
        "    df_tmp_f[\"wLotResp\"] = df_tmp_f[resp_col] - df_tmp_f[\"lotResp\"]\n",
        "\n",
        "    # Fit linear model\n",
        "    try:\n",
        "        model = correlation(df_tmp_f[ind_name], df_tmp_f[resp_col])\n",
        "        slp = model['slope']\n",
        "    except Exception:\n",
        "        slp = None\n",
        "\n",
        "    # Initialize slope counts\n",
        "    lvs = df_tmp_f[factor].unique()\n",
        "    num_slp = 0\n",
        "    num_cons_slp = 0\n",
        "\n",
        "    if slp is not None and len(lvs) > 1:\n",
        "        for lvl in lvs:\n",
        "            df_tmp_l = df_tmp_f[df_tmp_f[factor] == lvl]\n",
        "            if len(df_tmp_l) < 2:\n",
        "                continue\n",
        "            num_slp += 2\n",
        "\n",
        "            # Check individual slopes\n",
        "            try:\n",
        "                model_w = correlation(df_tmp_l[ind_name], df_tmp_l[resp_col])\n",
        "                if slp * model_w['slope'] > 0 and abs(model_w['slope']) > 0.05 * abs(slp):\n",
        "                    num_cons_slp += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # Check within-lot slopes\n",
        "            if len(df_tmp_l[\"wLotInd\"].unique()) >= 2:\n",
        "                try:\n",
        "                    model_wl = correlation(df_tmp_l[\"wLotInd\"], df_tmp_l[\"wLotResp\"])\n",
        "                    if slp * model_wl['slope'] > 0 and abs(model_wl['slope']) > 0.05 * abs(slp):\n",
        "                        num_cons_slp += 1\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            # Lot-level slopes\n",
        "            df_tmp_l_lot = df_tmp_l[[\"Lot\", \"lotInd\", \"lotResp\"]].drop_duplicates()\n",
        "            if len(df_tmp_l_lot) >= 2 and len(df_tmp_l_lot[\"lotInd\"].unique()) >= 2:\n",
        "                try:\n",
        "                    model_l = OLS(df_tmp_l_lot[\"lotResp\"], add_constant(df_tmp_l_lot[\"lotInd\"])).fit()\n",
        "                    if slp * model_l.params[1] > 0 and abs(model_l.params[1]) > 0.05 * abs(slp):\n",
        "                        num_cons_slp += 1\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    # Return slope multiplier\n",
        "    if num_slp > 0:\n",
        "        return num_cons_slp / num_slp\n",
        "    else:\n",
        "        return 0.75\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_neighbors(df_wafer, ind_name, response_col, ind_names, r_mag_limit=0.95):\n",
        "    \"\"\"\n",
        "    Find correlated neighbors for a given indicator in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        df_wafer (pd.DataFrame): Input data.\n",
        "        ind_name (str): Main indicator name.\n",
        "        response_col (str): Response variable name.\n",
        "        ind_names (list): List of all indicator names.\n",
        "        r_mag_limit (float): Threshold for correlation magnitude.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: DataFrame with correlation and p-value information, or None if no neighbors are found.\n",
        "    \"\"\"\n",
        "    # Ensure the main indicator is numeric and drop rows with NA in the indicator\n",
        "    df_wafer[ind_name] = pd.to_numeric(df_wafer[ind_name], errors='coerce')\n",
        "    df_wafer = df_wafer.dropna(subset=[ind_name])\n",
        "\n",
        "    if df_wafer[ind_name].nunique() == 1:\n",
        "        return None\n",
        "\n",
        "    response_col_new = response_col\n",
        "\n",
        "    # Find neighbors and their correlations\n",
        "    cor_vals = []\n",
        "    cor_inds = []\n",
        "\n",
        "    for cor_ind in ind_names:\n",
        "        if cor_ind == ind_name:\n",
        "            continue\n",
        "        df_wafer[cor_ind] = pd.to_numeric(df_wafer[cor_ind], errors='coerce')\n",
        "        if not df_wafer[cor_ind].isna().all():\n",
        "            valid_idx = ~df_wafer[cor_ind].isna()\n",
        "            cor_val, _ = pearsonr(df_wafer.loc[valid_idx, cor_ind], df_wafer.loc[valid_idx, ind_name])\n",
        "            cor_vals.append(cor_val)\n",
        "            cor_inds.append(cor_ind)\n",
        "\n",
        "    # Filter out NAs\n",
        "    cor_df = pd.DataFrame({\n",
        "        'MainInd': ind_name,\n",
        "        'CorrelatedInd': cor_inds,\n",
        "        'R.Val': cor_vals\n",
        "    })\n",
        "    cor_df['R.Mag'] = cor_df['R.Val'].abs()\n",
        "    cor_df = cor_df.sort_values(by='R.Mag', ascending=False)\n",
        "    cor_df = cor_df[(cor_df['R.Mag'] > 0.8) | (cor_df.index < 6)]\n",
        "\n",
        "    if cor_df.empty:\n",
        "        return None\n",
        "\n",
        "    # Add a column for p-values\n",
        "    p_value_col = f\"{response_col_new}.pValue\"\n",
        "    cor_df[p_value_col] = np.nan\n",
        "\n",
        "    for idx, row in cor_df.iterrows():\n",
        "        correlated_ind = row['CorrelatedInd']\n",
        "        df_wafer[correlated_ind] = pd.to_numeric(df_wafer[correlated_ind], errors='coerce')\n",
        "        valid_wafer = df_wafer.dropna(subset=[response_col_new, correlated_ind])\n",
        "\n",
        "        if valid_wafer.empty:\n",
        "            cor_df.loc[idx, p_value_col] = np.nan\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            model = ols(f\"{response_col_new} ~ {correlated_ind}\", data=valid_wafer).fit()\n",
        "            cor_df.loc[idx, p_value_col] = model.pvalues[correlated_ind]\n",
        "        except:\n",
        "            cor_df.loc[idx, p_value_col] = np.nan\n",
        "\n",
        "    # Rename the p-value column to 'pval'\n",
        "    cor_df.rename(columns={p_value_col: 'pval'}, inplace=True)\n",
        "    return cor_df\n",
        "\n",
        "\n",
        "\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "def calc_neighbors_by_influence(df, df_grades, ind_names, resp_col, rsq_red_spec=0.50, pres=None):\n",
        "    \"\"\"\n",
        "    Calculate influence of neighbors on the response variable.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input data.\n",
        "        df_grades (pd.DataFrame): Grades DataFrame with indicator details.\n",
        "        ind_names (list): List of indicator names.\n",
        "        resp_col (str): Response column name.\n",
        "        rsq_red_spec (float): R-squared reduction threshold.\n",
        "        pres (optional): Not used in this implementation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated grades DataFrame with influence information.\n",
        "    \"\"\"\n",
        "    # Sort grades by 'Grade' in descending order\n",
        "    df_grades = df_grades.sort_values(by='Grade', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Return immediately if there are fewer than 2 indicators\n",
        "    if len(ind_names) < 2:\n",
        "        return df_grades\n",
        "\n",
        "    # Normalize df for all indicators by factor\n",
        "    df_tmp = df.copy()\n",
        "    for ind in ind_names:\n",
        "        df_tmp = NormalizeByFactor(df_tmp, resp_col, ind, \"UniqueModule\")\n",
        "\n",
        "    # Save the normalized data for reuse\n",
        "    df_tmp_save = df_tmp.copy()\n",
        "\n",
        "    for i in range(1, len(ind_names)):\n",
        "        for j in range(i):\n",
        "            # Filter rows where neither indicator is NaN\n",
        "            valid_rows = df_tmp_save[df_tmp_save[df_grades.loc[i, 'X']].notna() &\n",
        "                                     df_tmp_save[df_grades.loc[j, 'X']].notna()]\n",
        "            if len(valid_rows) < 5:\n",
        "                continue\n",
        "\n",
        "            if df_grades.loc[j, 'Nebr.infl'] != df_grades.loc[j, 'X']:\n",
        "                continue\n",
        "\n",
        "            if valid_rows[df_grades.loc[i, 'X']].nunique() == 1 or valid_rows[df_grades.loc[j, 'X']].nunique() == 1:\n",
        "                continue\n",
        "\n",
        "            # Fit base model\n",
        "            try:\n",
        "                out_base = ols(f\"{resp_col} ~ {df_grades.loc[i, 'X']}\", data=valid_rows).fit()\n",
        "                rsq_base = out_base.rsquared\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Fit neighbor model\n",
        "            try:\n",
        "                out_nebr = ols(f\"{resp_col} ~ {df_grades.loc[j, 'X']}\", data=valid_rows).fit()\n",
        "                valid_rows['resp_resid'] = out_nebr.resid\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Fit residual model\n",
        "            try:\n",
        "                out_resid = ols(f\"resp_resid ~ {df_grades.loc[i, 'X']}\", data=valid_rows).fit()\n",
        "                rsq_adj_base = out_resid.rsquared\n",
        "                rsq_red = 1 - (rsq_adj_base / rsq_base)\n",
        "            except:\n",
        "                rsq_adj_base = None\n",
        "                rsq_red = None\n",
        "\n",
        "            # Update grades if reduction exceeds threshold\n",
        "            if rsq_red is not None and rsq_red > rsq_red_spec:\n",
        "                df_grades.loc[i, 'Nebr.infl'] = df_grades.loc[j, 'X']\n",
        "                df_grades.loc[i, 'Nebr.inflRed'] = rsq_red\n",
        "                break\n",
        "\n",
        "    return df_grades\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def compute_neighbors_just_one(df_wafer, ind_name, response_col, ind_names, r_mag_limit=0.95):\n",
        "    \"\"\"\n",
        "    Identify the first neighbor with a correlation above the threshold.\n",
        "\n",
        "    Parameters:\n",
        "        df_wafer (pd.DataFrame): Input DataFrame.\n",
        "        ind_name (str): Name of the main indicator.\n",
        "        response_col (str): Response variable name.\n",
        "        ind_names (list): List of all indicator names.\n",
        "        r_mag_limit (float): Threshold for correlation magnitude.\n",
        "\n",
        "    Returns:\n",
        "        str: Name of the correlated indicator or the original indicator name if no match is found.\n",
        "    \"\"\"\n",
        "    # Drop rows where the main indicator is NaN\n",
        "    df_wafer = df_wafer.dropna(subset=[ind_name])\n",
        "\n",
        "    # If the indicator has only one unique value, return the indicator name\n",
        "    if df_wafer[ind_name].nunique() == 1:\n",
        "        return ind_name\n",
        "\n",
        "    # Iterate through other indicators to find neighbors\n",
        "    for cor_ind in ind_names:\n",
        "        if cor_ind == ind_name:\n",
        "            continue\n",
        "\n",
        "        # Ensure the correlated indicator is numeric and not NaN\n",
        "        if cor_ind in df_wafer.columns:\n",
        "            valid_idx = ~df_wafer[cor_ind].isna()\n",
        "            if valid_idx.sum() < 2:  # Require at least two valid points\n",
        "                continue\n",
        "\n",
        "            # Calculate Pearson correlation\n",
        "            try:\n",
        "                cor_val, _ = pearsonr(df_wafer.loc[valid_idx, cor_ind], df_wafer.loc[valid_idx, ind_name])\n",
        "            except ValueError:  # Handle cases where correlation can't be computed\n",
        "                continue\n",
        "\n",
        "            # Check if correlation exceeds the threshold\n",
        "            if not np.isnan(cor_val) and cor_val > r_mag_limit:\n",
        "                return cor_ind\n",
        "\n",
        "    # Return the original indicator name if no neighbor exceeds the threshold\n",
        "    return ind_name\n",
        "\n",
        "def parameter_correlation(table, mapping_table, resp_col_index, split_col_index, all_inds, num_periods, cutoff_pval, pft_constant=\"PFTConstant\", is_indicator_data=False, need_group_summary=False):\n",
        "    \"\"\"\n",
        "    Perform parameter correlation analysis based on indicators and response variables.\n",
        "\n",
        "    Parameters:\n",
        "        table (pd.DataFrame): Main data table.\n",
        "        mapping_table (pd.DataFrame): Mapping table containing metadata.\n",
        "        resp_col_index (int): Index of the response column.\n",
        "        split_col_index (int or None): Index of the split column (if any).\n",
        "        ind_cols_index (list): List of indicator column indices.\n",
        "        num_periods (int): Number of periods to split the data into.\n",
        "        cutoff_pval (float): p-value cutoff for statistical significance.\n",
        "        pft_constant (str): Column name for PFT constant in the mapping table.\n",
        "        is_indicator_data (bool): Whether the data is indicator data.\n",
        "        need_group_summary (bool): Whether to compute group summaries.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing correlation tables, neighbors, PFT table, and group summary table.\n",
        "    \"\"\"\n",
        "    stats = [\"MeanT\", \"Min\", \"Max\", \"StdDev\", \"Area\", \"Duration\", \"Median\", \"Range\"]\n",
        "    stat_weights = [1, 0.9, 0.9, 0.5, 0.7, 1, 0.7, 0.8]\n",
        "    pfts = mapping_table[pft_constant].dropna().drop_duplicates()\n",
        "\n",
        "    lot_col=\"Lot\"\n",
        "    wf_col=\"Wafer\"\n",
        "\n",
        "    # Normalize columns for non-indicator data\n",
        "    if not is_indicator_data:\n",
        "        for pft in pfts:\n",
        "            block_id = mapping_table.loc[mapping_table[pft_constant] == pft, \"blockID\"].iloc[0]\n",
        "            for col_type in [\"Chamber\", \"dxcontextid\", \"fdccontextid\", \"pequipment\"]:\n",
        "                col = mapping_table.loc[(mapping_table[pft_constant] == pft) & (mapping_table[\"Type\"] == col_type), \"columnName\"]\n",
        "                if col.empty:\n",
        "                    col_name = f\"{pft}_{col_type}\"\n",
        "                    table[col_name] = 0\n",
        "                    mapping_table = pd.concat([mapping_table, pd.DataFrame({\n",
        "                        \"columnName\": [col_name],\n",
        "                        pft_constant: [pft],\n",
        "                        \"blockID\": [block_id],\n",
        "                        \"Type\": [col_type]\n",
        "                    })])\n",
        "\n",
        "    response_col = table.columns[resp_col_index]\n",
        "    # all_inds = table.columns[ind_cols_index]\n",
        "    split_col = None if split_col_index is None else table.columns[split_col_index]\n",
        "    r_mag_limit = 0.95\n",
        "    cutoff_pval = float(cutoff_pval)\n",
        "    num_periods = int(num_periods)\n",
        "\n",
        "    cor_res_all = None\n",
        "    nebrs_all = None\n",
        "    pft_table_all = None\n",
        "    cor_group_res = None\n",
        "\n",
        "    # Prepare the data if split column exists\n",
        "    if split_col:\n",
        "        table[\"oldLot\"] = table[split_col]\n",
        "        table[\"oldWafer\"] = table[split_col]\n",
        "        table[\"lotCol\"] = table[split_col] + \"_\" + table[\"oldLot\"]\n",
        "        table[\"wfCol\"] = table[split_col] + \"_\" + table[\"oldWafer\"]\n",
        "\n",
        "    for pft in pfts:\n",
        "        # print(pft)\n",
        "        if not pft:\n",
        "            continue\n",
        "\n",
        "        pft_cols = mapping_table.loc[mapping_table[pft_constant] == pft, \"columnName\"]\n",
        "        pft_cols = pft_cols[pft_cols.isin(table.columns)]\n",
        "\n",
        "        if len(pft_cols) == 0:\n",
        "            continue\n",
        "\n",
        "        df_ind = table[[response_col] + pft_cols.tolist()]\n",
        "        df_ind = df_ind.dropna(subset=[response_col])\n",
        "\n",
        "        if df_ind.empty:\n",
        "            continue\n",
        "\n",
        "        ind_names = [col for col in all_inds if col in pft_cols.tolist()]\n",
        "        # print(ind_names)\n",
        "        if not ind_names:\n",
        "            continue\n",
        "\n",
        "        # Time column\n",
        "        time_col = mapping_table.loc[(mapping_table[pft_constant] == pft) & (mapping_table[\"Type\"] == \"start_time\"), \"columnName\"]\n",
        "        if time_col.empty:\n",
        "            continue\n",
        "        time_col = time_col.iloc[0]\n",
        "\n",
        "        df_ind[\"timeCol1\"] = pd.to_datetime(df_ind[time_col], errors=\"coerce\")\n",
        "        df_ind = df_ind.dropna(subset=[\"timeCol1\"])\n",
        "        if df_ind.empty:\n",
        "            continue\n",
        "\n",
        "        df_ind = df_ind.sort_values(by=\"timeCol1\")\n",
        "        time_vals = np.quantile(df_ind[\"timeCol1\"].values.astype(np.int64), q=np.linspace(0, 1, num_periods + 1)[1:])\n",
        "        df_ind[\"TimePeriod\"] = pd.cut(df_ind[\"timeCol1\"].values.astype(np.int64), bins=[-np.inf] + time_vals.tolist(), labels=False)\n",
        "\n",
        "        # Add UniqueModule column\n",
        "        chamber_col = mapping_table.loc[(mapping_table[pft_constant] == pft) & (mapping_table[\"Type\"] == \"Chamber\"), \"columnName\"].iloc[0]\n",
        "        eqp_col = mapping_table.loc[(mapping_table[pft_constant] == pft) & (mapping_table[\"Type\"] == \"pequipment\"), \"columnName\"].iloc[0]\n",
        "        df_ind[\"UniqueModule\"] = df_ind[eqp_col].astype(str) + \":\" + df_ind[chamber_col].astype(str)\n",
        "\n",
        "        # Perform correlation analysis for each indicator\n",
        "        cor_res = []\n",
        "        for ind_name in ind_names:\n",
        "            try:\n",
        "                if df_ind[ind_name].isna().all():\n",
        "                    continue\n",
        "\n",
        "                tmp = CheckCorrGrade(\n",
        "                    df_ind, ind_name, lot_col, wf_col, response_col=response_col,\n",
        "                    factors=[\"UniqueModule\", \"TimePeriod\"], cutoff_p_val=cutoff_pval\n",
        "                )\n",
        "                if tmp is not None:\n",
        "                    cor_res.append(tmp)\n",
        "            except Exception as e:\n",
        "                a = e # need to be modified\n",
        "\n",
        "        if cor_res:\n",
        "            # cor_res = pd.concat(cor_res, ignore_index=True)\n",
        "            cor_res = pd.DataFrame(cor_res)\n",
        "            cor_res.sort_values(by=\"Grade\", ascending=False, inplace=True)\n",
        "            cor_res[\"Nebr\"] = cor_res[\"X\"]\n",
        "            cor_res[pft_constant] = pft\n",
        "\n",
        "        # Additional correlation adjustments\n",
        "        if cor_res is not None:\n",
        "            cor_res[\"Grade.UnWeighted\"] = cor_res[\"Grade\"]\n",
        "            for stat, weight in zip(stats, stat_weights):\n",
        "                mask = cor_res[\"X\"].str.contains(stat) & cor_res[\"Grade.UnWeighted\"].notna()\n",
        "                cor_res.loc[mask, \"Grade\"] *= weight\n",
        "            cor_res.sort_values(by=\"Grade\", ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "        all_cor_indx = None\n",
        "        if cor_res is not None:\n",
        "            cor_res = cor_res.sort_values(by=\"Grade\", ascending=False)\n",
        "            cor_res[\"Nebr\"] = cor_res[\"X\"]\n",
        "            cor_res[pft_constant] = pft\n",
        "\n",
        "            sig_df = cor_res[cor_res[\"Class\"] != \"\"]\n",
        "            if not sig_df.empty:\n",
        "                for k in range(len(sig_df)):\n",
        "                    df_wafer = df_ind.copy()\n",
        "                    x_col = sig_df.iloc[k, 0]  # first row\n",
        "                    ind_name = x_col\n",
        "\n",
        "                    # Compute neighbors\n",
        "                    cor_indx = compute_neighbors(df_ind, ind_name, response_col, ind_names, r_mag_limit)\n",
        "                    cor_indx[pft_constant] = pft\n",
        "                    cor_indx[\"Y\"] = response_col\n",
        "\n",
        "                    # Append to AllCorIndx\n",
        "                    if all_cor_indx is None:\n",
        "                        all_cor_indx = cor_indx\n",
        "                    else:\n",
        "                        all_cor_indx = pd.concat([all_cor_indx, cor_indx], ignore_index=True)\n",
        "\n",
        "                    if k > 0:\n",
        "                        sig_inds = sig_df[\"X\"].iloc[:k].tolist()\n",
        "                        nebr = compute_neighbors_just_one(df_ind, ind_name, response_col, sig_inds, r_mag_limit)\n",
        "                        cor_res.loc[cor_res[\"X\"] == ind_name, \"Nebr\"] = nebr\n",
        "\n",
        "        if cor_res is not None:\n",
        "            cor_res[\"Grade.UnWeighted\"] = cor_res[\"Grade\"]\n",
        "            stats_weighted = [False] * len(cor_res)\n",
        "\n",
        "            for i, stat in enumerate(stats):\n",
        "                mask = cor_res[\"X\"].str.contains(stat) & ~pd.Series(stats_weighted)\n",
        "                stats_weighted = stats_weighted | mask\n",
        "                cor_res.loc[mask, \"Grade\"] *= stat_weights[i]\n",
        "\n",
        "            cor_res = cor_res.sort_values(by=\"Grade\", ascending=False)\n",
        "            cor_res[\"Nebr.infl\"] = cor_res[\"X\"]\n",
        "            cor_res[\"Nebr.inflRed\"] = None\n",
        "            sig_inds = cor_res.loc[cor_res[\"Grade\"] > 0, \"X\"].tolist()\n",
        "\n",
        "            # Calculate neighbors by influence\n",
        "            cor_res = calc_neighbors_by_influence(df_ind, cor_res, sig_inds, response_col, rsq_red_spec=0.5, pres=None)\n",
        "\n",
        "            # cor_res = cor_res[[\n",
        "            #     \"X\", \"Grade\", \"Nebr.infl\", \"Nebr.inflRed\", \"Class\", \"Note\", \"Grade.UnWeighted\",\n",
        "            #     \"pval\", \"pval.L\", \"pval.WL\", \"base.grade\", \"adj.grade\", \"best.grade\", \"slope\",\n",
        "            #     \"slope.L\", \"slope.WL\", \"Nebr\", \"Y\", pft_constant\n",
        "            # ]]\n",
        "            cor_res = cor_res[[\n",
        "                \"X\", \"Grade\", \"Nebr.infl\", \"Nebr.inflRed\", \"Class\", \"Note\", \"Grade.UnWeighted\",\n",
        "                \"pval\", \"pval.L\", \"pval.WL\", \"slope\",\n",
        "                \"slope.L\", \"slope.WL\", \"Nebr\", \"Y\", pft_constant\n",
        "            ]]\n",
        "\n",
        "            num_sig = (cor_res[\"Class\"] != \"\").sum()\n",
        "            num_parameters = len(cor_res)\n",
        "            pft_table = pd.DataFrame([{\n",
        "                pft_constant: pft,\n",
        "                \"SignificantParameters\": num_sig,\n",
        "                \"TotalParameters\": num_parameters,\n",
        "                \"TaggedParameters\": 0,\n",
        "                \"Comments\": \"\",\n",
        "                \"numWafers\": 0,\n",
        "                \"uniqueWafers\": 0\n",
        "            }])\n",
        "\n",
        "            if pft_table_all is None:\n",
        "                pft_table_all = pft_table\n",
        "            else:\n",
        "                pft_table_all = pd.concat([pft_table_all, pft_table], ignore_index=True)\n",
        "\n",
        "            if cor_res_all is None:\n",
        "                cor_res_all = cor_res\n",
        "            else:\n",
        "                cor_res_all = pd.concat([cor_res_all, cor_res], ignore_index=True)\n",
        "\n",
        "            if nebrs_all is None:\n",
        "                nebrs_all = all_cor_indx\n",
        "            else:\n",
        "                nebrs_all = pd.concat([nebrs_all, all_cor_indx], ignore_index=True)\n",
        "\n",
        "        # Group Correlations\n",
        "\n",
        "        all_group_cor_indx = None\n",
        "        if cor_group_res is not None:\n",
        "            cor_group_res = cor_group_res.sort_values(by=\"Grade\", ascending=False)\n",
        "            cor_group_res[\"Nebr\"] = cor_group_res[\"X\"]\n",
        "            cor_group_res[pft_constant] = pft\n",
        "\n",
        "            sig_df = cor_group_res[cor_group_res[\"Class\"] != \"\"]\n",
        "            if not sig_df.empty:\n",
        "                for k in range(len(sig_df)):\n",
        "                    df_wafer = df_ind.copy()\n",
        "                    x_col = sig_df.iloc[k, 0]\n",
        "                    ind_name = x_col\n",
        "\n",
        "                    cor_indx = compute_neighbors(df_ind, ind_name, response_col, ind_names, r_mag_limit)\n",
        "                    cor_indx[pft_constant] = pft\n",
        "                    cor_indx[\"Y\"] = response_col\n",
        "\n",
        "                    if all_group_cor_indx is None:\n",
        "                        all_group_cor_indx = cor_indx\n",
        "                    else:\n",
        "                        all_group_cor_indx = pd.concat([all_group_cor_indx, cor_indx], ignore_index=True)\n",
        "\n",
        "                    if k > 0:\n",
        "                        sig_inds = sig_df[\"X\"].iloc[:k].tolist()\n",
        "                        nebr = compute_neighbors_just_one(df_ind, ind_name, response_col, sig_inds, r_mag_limit)\n",
        "                        cor_group_res.loc[cor_group_res[\"X\"] == ind_name, \"Nebr\"] = nebr\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"CorTable\": cor_res_all,\n",
        "        \"Nebrs\": nebrs_all,\n",
        "        \"PFTTable\": pft_table_all,\n",
        "        \"GroupSummaryTable\": cor_group_res\n",
        "    }\n"
      ],
      "metadata": {
        "id": "pVB4MlUQcvGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}