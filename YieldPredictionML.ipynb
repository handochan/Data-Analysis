{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Using cached pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.22.3-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     ---------------------------------------- 14.7/14.7 MB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "Successfully installed numpy-1.22.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Collecting pandas\n",
      "Note: you may need to restart the kernel to use updated packages.  Downloading pandas-1.4.1-cp39-cp39-win_amd64.whl (10.5 MB)\n",
      "     --------------------------------------- 10.5/10.5 MB 58.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "Successfully installed pandas-1.4.1\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=f55d60051a7745a6d0d78babf5ebf62a7f5d4e11467dcb040ce99868b220dae5\n",
      "  Stored in directory: c:\\users\\dochanh\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (1.22.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dochanh\\anaconda3\\lib\\site-packages (from xgboost) (1.22.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dochanh\\Anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set, Feature & Label\n",
    "dataset = pd.read_csv('D:/dataset/PCM_group1.csv') #encoding=\"UTF16\"\n",
    "#dataset.dropna(inplace=True) # 결측치가 존재하는 row 삭제\n",
    "\n",
    "# Data Preprocessing\n",
    "x = dataset.filter(regex='avg -|std -')\n",
    "\n",
    "# yield - log scaling\n",
    "org_y = dataset.filter(regex='yield')\n",
    "y = 100.1 - org_y\n",
    "y = np.log(y)\n",
    "dataset['log_yield'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 213)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling, Outlier, Customizing obj&eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거\n",
    "def remove_out(dataframe, remove_col):\n",
    "    dff = dataframe\n",
    "    for k in remove_col:\n",
    "        level_1q = dff[k].quantile(0.25)\n",
    "        level_3q = dff[k].quantile(0.75)\n",
    "        IQR = level_3q - level_1q\n",
    "        rev_range = 6 # 제거 범위 조절 변수\n",
    "        dff = dff[(dff[k] <= level_3q + (rev_range*IQR)) & (dff[k] >= level_1q - (rev_range*IQR))]\n",
    "        dff = dff.reset_index(drop=True)\n",
    "    return dff\n",
    "\n",
    "X_train.shape\n",
    "col_list = list(x.columns)\n",
    "non_outlier = remove_out(dataset, col_list)\n",
    "non_outlier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-1 Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "scaled_x = scaler.transform(x)\n",
    "scaled_x = pd.DataFrame(scaled_x)\n",
    "\n",
    "from pickle import dump\n",
    "dump(scaler, open('C:/', 'wb'))\n",
    "\n",
    "from pickle import load\n",
    "mm_scaler = load(open('', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function, eval_metric -> 오차 네 제곱으로 Customizing\n",
    "def custom_objective(labels, preds):\n",
    "    grad = 4*(preds - labels)**3\n",
    "    hess = 12*(preds - labels)**2 #grad를 미분한것이 hess\n",
    "    return grad, hess\n",
    "\n",
    "def custom_evaluation(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return \"custom_error\", ((np.subtract(preds, labels))**4).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0) #scaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default, baseline\n",
    "xgb_model = xgboost.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.5, enable_categorical=False,\n",
       "             gamma=0.5, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.08, max_delta_step=0,\n",
       "             max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=1500, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Design & learning\n",
    "xgb_model = xgboost.XGBRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.08,\n",
    "    gamma=0.5,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    max_depth=6,\n",
    "    verbosity=1)\n",
    "    # objective = custom_objective\n",
    "    # eval_metric = custom_evaluation\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.save_model('C:/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom score\n",
    "def accuracy(predictions, real_y):\n",
    "    residual = abs(predictions - real_y)\n",
    "    good = residual <= 1.5\n",
    "    good = good.loc[good==True]\n",
    "    bad = residual > 1.5\n",
    "    bad = bad.loc[bad == True]\n",
    "    score = len(good)/(len(good)+len(bad))\n",
    "    return score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5663247133597713\n",
      "-2.9901889194550857\n",
      "593.534230305082\n",
      "24.362557958988667\n",
      "17.78959092625809\n"
     ]
    }
   ],
   "source": [
    "predictions = xgb_model.predict(X_test)\n",
    "real_predictions = np.exp(predictions)\n",
    "real_predictions = 100.1 - real_predictions\n",
    "\n",
    "real_y_test = np.exp(y_test)\n",
    "real_y_test = 100.1  - real_y_test\n",
    "\n",
    "r_sq = xgb_model.score(X_train, y_train)\n",
    "ev_score = explained_variance_score(real_predictions, real_y_test)\n",
    "mse = mean_squared_error(real_y_test, real_predictions)\n",
    "rmse = mse**(1/2.0)\n",
    "mae = mean_absolute_error(real_y_test, real_predictions)\n",
    "#acc = accuracy(real_y_test, predictions)\n",
    "\n",
    "print(r_sq, ev_score, mse, rmse, mae, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_parameter_bound = {\n",
    "    'gamma':(0,5),\n",
    "    'max_depth':(4,7),\n",
    "    'subsample':(0.5,1),\n",
    "    'learning_rate':(0.01,0.2),\n",
    "    'colsample_bytree':(0.5,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_bo(gamma, max_depth, subsample, learning_rate, colsample_bytree):\n",
    "    xgb_params = {\n",
    "        'gamma':int(round(gamma)),\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'subsample':int(round(subsample)),\n",
    "        'learning_rate':int(round(learning_rate)),\n",
    "        'colsample_bytree':int(round(colsample_bytree))\n",
    "    }\n",
    "    xgb = xgboost.XGBRegressor(**xgb_params, n_estimators=1500)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.1)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = xgb.predict(X_valid)\n",
    "    \n",
    "    score = -mean_squared_error(y_valid, predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "BO_xgb = BayesianOptimization(f=xgb_bo, pbounds=xgb_parameter_bound, random_state=0)\n",
    "BO_xgb.maximize(init_points=5, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 결과값\n",
    "\n",
    "model = xgboost.XGBRegressor()\n",
    "model.load_model(\"C:/\")\n",
    "\n",
    "testdata = pd.read_csv('C:/')\n",
    "testdata.dropna(inplace=True)\n",
    "test = testdata.filter(regex='avg -|std -')\n",
    "scaled_test = mm_scaler.transform(test)\n",
    "\n",
    "testdata['Used Model'] = 'Program_Name'\n",
    "testdata['retrained flag'] = 0\n",
    "\n",
    "predictions = model.predict(scaled_test)\n",
    "real_predictions = np.exp(predictions)\n",
    "real_predictions = 100.1 - real_predictions\n",
    "\n",
    "feature_importance = pd.DataFrame({\"Feature\": test.columns,\n",
    "                                   \"Importance\": model.feature_importances_})\n",
    "\n",
    "df_prediction = pd.DataFrame(real_predictions, columns=['predict'])\n",
    "results_set = pd.concat([testdata['Used Model'], testdata['Lot'], testdata['Wafer'], df_prediction, testdata['retrained flag']], axis=1)\n",
    "results_set.insert(4, 'yield (%)','')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05e0508406940368946a2f4cb4f2279d6d1b2c8f9b357e2e2e71380a4ca8f258"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
